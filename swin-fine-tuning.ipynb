{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Fine Tuning of swin model and validation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import modal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "from transformers import AutoImageProcessor\n",
    "from transformers import AutoModelForImageClassification\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Modal setup\n",
    "stub = modal.App(\"swinv2-fairface-training\")\n",
    "\n",
    "# Define the image for Modal\n",
    "image = modal.Image.debian_slim().pip_install(\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"transformers\",\n",
    "    \"pandas\",\n",
    "    \"pillow\",\n",
    "    \"tqdm\",\n",
    "    \"pyarrow\",\n",
    "    \"accelerate\"\n",
    ")\n",
    "\n",
    "# Create volumes\n",
    "volume = modal.Volume.from_name(\"fairface-data\", create_if_missing=True)\n",
    "model_volume = modal.Volume.from_name(\"swinv2-models\", create_if_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb5389f5791413d98dc54c06a8a4069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/240 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\py39\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\monis\\.cache\\huggingface\\hub\\models--microsoft--swinv2-base-patch4-window16-256. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from accelerate import Accelerator\n",
    "def create_swinv2_model(num_classes):\n",
    "    model = AutoModelForImageClassification.from_pretrained(\n",
    "        'microsoft/swinv2-base-patch4-window16-256',\n",
    "        num_labels=num_classes,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Get processor and normalization values\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-base-patch4-window16-256\")\n",
    "mean = processor.image_mean\n",
    "std = processor.image_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for FairFace\n",
    "class FairFaceDataset(Dataset):\n",
    "    def __init__(self, parquet_file, transform=None):\n",
    "        self.data = pd.read_parquet(parquet_file)\n",
    "        # Use processor's normalization\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image = Image.open(io.BytesIO(row['image']['bytes'])).convert('RGB')\n",
    "        label = row['race']  # Adjust if your label column is different\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "def get_dataloaders(batch_size=32):\n",
    "    train_dataset = FairFaceDataset(\"/root/data/train.parquet\")\n",
    "    val_dataset = FairFaceDataset(\"/root/data/validation.parquet\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, accelerator, print_every=10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        if (batch_idx + 1) % print_every == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            accelerator.print(f\"Batch {batch_idx+1}/{len(train_loader)} - \"\n",
    "                              f\"Train Loss: {total_loss/(batch_idx+1):.4f}, \"\n",
    "                              f\"Train Acc: {100.*correct/total:.2f}%\")\n",
    "\n",
    "    return total_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "def validate(model, val_loader, criterion, accelerator, print_every=10):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(val_loader):\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            if (batch_idx + 1) % print_every == 0 or (batch_idx + 1) == len(val_loader):\n",
    "                accelerator.print(f\"Batch {batch_idx+1}/{len(val_loader)} - \"\n",
    "                                  f\"Val Loss: {total_loss/(batch_idx+1):.4f}, \"\n",
    "                                  f\"Val Acc: {100.*correct/total:.2f}%\")\n",
    "\n",
    "    return total_loss / len(val_loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@stub.function(\n",
    "    image=image,\n",
    "    gpu=\"A100\",  # Use 1 A100 GPU\n",
    "    volumes={\"/root/data\": volume, \"/root/models\": model_volume},\n",
    "    timeout=14400\n",
    ")\n",
    "def train_swinv2_model(num_epochs=10, batch_size=16, learning_rate=2e-5):\n",
    "    accelerator = Accelerator()\n",
    "    accelerator.print(f\"Accelerator process {accelerator.process_index} of {accelerator.num_processes} on device {accelerator.device}\")\n",
    "\n",
    "    train_loader, val_loader = get_dataloaders(batch_size)\n",
    "    num_classes = 7  # Adjust if your dataset has a different number of classes\n",
    "    model = create_swinv2_model(num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Prepare for distributed training (even if using 1 GPU, this is fine)\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        accelerator.print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, accelerator)\n",
    "        accelerator.print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, accelerator)\n",
    "        accelerator.print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            accelerator.save(model.state_dict(), \"/root/models/swinv2_fairface_best.pth\")\n",
    "            accelerator.print(f\"Saved new best model with validation accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    accelerator.save(model.state_dict(), \"/root/models/swinv2_fairface_final.pth\")\n",
    "    accelerator.print(\"\\nTraining completed!\")\n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "with stub.run():\n",
    "    best_acc = train_swinv2_model.remote(num_epochs=10, batch_size=16)\n",
    "    print(f\"Best validation accuracy: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To carry out validation seperately for base swin model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Accuracy: 71.94%\n"
     ]
    }
   ],
   "source": [
    "import modal\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "from tqdm import tqdm\n",
    "\n",
    "stub = modal.App(\"swin-pth-validation\")\n",
    "\n",
    "image = modal.Image.debian_slim().pip_install(\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"transformers\",\n",
    "    \"pandas\",\n",
    "    \"pillow\",\n",
    "    \"pyarrow\",\n",
    "    \"tqdm\"\n",
    ")\n",
    "\n",
    "data_volume = modal.Volume.from_name(\"fairface-data\")\n",
    "model_volume = modal.Volume.from_name(\"swinv2-models\")\n",
    "output_volume = modal.Volume.from_name(\"swin-validation-output\", create_if_missing=True)\n",
    "\n",
    "class FairFaceDataset(Dataset):\n",
    "    def __init__(self, parquet_file, transform=None):\n",
    "        self.data = pd.read_parquet(parquet_file)\n",
    "        processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-base-patch4-window16-256\")\n",
    "        mean = processor.image_mean\n",
    "        std = processor.image_std\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image = Image.open(io.BytesIO(row['image']['bytes'])).convert('RGB')\n",
    "        label = row['race']\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "@stub.function(\n",
    "    image=image,\n",
    "    gpu=\"T4\",\n",
    "    timeout=1800,\n",
    "    volumes={\n",
    "        \"/data\": data_volume,\n",
    "        \"/models\": model_volume,\n",
    "        \"/output\": output_volume\n",
    "    }\n",
    ")\n",
    "def validate_swin_pth(\n",
    "    pth_model_path=\"/models/swinv2_fairface_best.pth\",\n",
    "    parquet_path=\"/data/validation.parquet\",\n",
    "    output_csv=\"/output/swin_val_predictions_base.csv\",\n",
    "    batch_size=32,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    fairface_classes = [\n",
    "        \"White\", \"Black\", \"Latino_Hispanic\", \"East Asian\",\n",
    "        \"Southeast Asian\", \"Indian\", \"Middle Eastern\"\n",
    "    ]\n",
    "    num_classes = 7\n",
    "    model = AutoModelForImageClassification.from_pretrained(\n",
    "        'microsoft/swinv2-base-patch4-window16-256',\n",
    "        num_labels=num_classes,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    model.load_state_dict(torch.load(pth_model_path, map_location=device))\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    val_dataset = FairFaceDataset(parquet_path)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            top5_probs, top5_preds = probs.topk(5, dim=1)\n",
    "            top1_pred = top5_preds[:, 0]\n",
    "            top1_prob = top5_probs[:, 0]\n",
    "            _, predicted = probs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            for i in range(len(labels)):\n",
    "                all_predictions.append({\n",
    "                    'true_label': fairface_classes[labels[i].item()],\n",
    "                    'predicted_label': fairface_classes[predicted[i].item()],\n",
    "                    'confidence': probs[i][predicted[i]].item(),\n",
    "                    'top1_pred': fairface_classes[top1_pred[i].item()],\n",
    "                    'top1_prob': top1_prob[i].item(),\n",
    "                    'top5_preds': [fairface_classes[idx] for idx in top5_preds[i].cpu().numpy()],\n",
    "                    'top5_probs': top5_probs[i].cpu().numpy().tolist()\n",
    "                })\n",
    "\n",
    "    accuracy = 100. * correct / total\n",
    "    results_df = pd.DataFrame(all_predictions)\n",
    "    results_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Predictions saved to {output_csv}\")\n",
    "    return accuracy\n",
    "\n",
    "# Run validation\n",
    "with stub.run():\n",
    "    acc = validate_swin_pth.remote()\n",
    "    print(f\"Final Validation Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
