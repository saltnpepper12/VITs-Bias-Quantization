{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation of Quantized VIT model on validation split from Hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Modal Setup\n",
    "import modal\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import onnxruntime as ort\n",
    "import base64\n",
    "\n",
    "# Modal setup\n",
    "stub = modal.App(\"vit-fairface-validation\")\n",
    "\n",
    "# Define the image for Modal\n",
    "image = modal.Image.debian_slim().pip_install(\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"pandas\",\n",
    "    \"pillow\",\n",
    "    \"tqdm\",\n",
    "    \"pyarrow\",\n",
    "    \"onnxruntime-gpu\"\n",
    ")\n",
    "\n",
    "# Create volumes\n",
    "data_volume = modal.Volume.from_name(\"fairface-data\")\n",
    "output_volume = modal.Volume.from_name(\"vit-validation-output\", create_if_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dataset Class\n",
    "class FairFaceDataset(Dataset):\n",
    "    def __init__(self, parquet_file, transform=None):\n",
    "        self.data = pd.read_parquet(parquet_file)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image = Image.open(io.BytesIO(row['image']['bytes'])).convert('RGB')\n",
    "        label = row['race']\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Validation Function\n",
    "@stub.function(\n",
    "    image=image,\n",
    "    gpu=\"A100\",\n",
    "    volumes={\n",
    "        \"/root/data\": data_volume,\n",
    "        \"/root/output\": output_volume\n",
    "    },\n",
    "    timeout=14400\n",
    ")\n",
    "def validate_quantized_model(batch_size=32, model_bytes=None):\n",
    "    import onnxruntime as ort\n",
    "    \n",
    "    # Class mapping for FairFace\n",
    "    fairface_classes = [\n",
    "        \"White\", \"Black\", \"Latino_Hispanic\", \"East Asian\",\n",
    "        \"Southeast Asian\", \"Indian\", \"Middle Eastern\"\n",
    "    ]\n",
    "    \n",
    "    # Create ONNX Runtime session from bytes\n",
    "    session = ort.InferenceSession(\n",
    "        model_bytes,\n",
    "        providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "    )\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    val_dataset = FairFaceDataset(\"/root/data/validation.parquet\")\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize metrics\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Validation loop\n",
    "# Validation loop\n",
    "    for images, labels in tqdm(val_loader):\n",
    "        images_np = images.numpy()\n",
    "        outputs = session.run(\n",
    "            None,\n",
    "            {'pixel_values': images_np}\n",
    "        )[0]\n",
    "        probs = torch.softmax(torch.tensor(outputs), dim=1)\n",
    "        top5_probs, top5_preds = probs.topk(5, dim=1)\n",
    "        top1_pred = top5_preds[:, 0]\n",
    "        top1_prob = top5_probs[:, 0]\n",
    "        _, predicted = probs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            all_predictions.append({\n",
    "                'true_label': fairface_classes[labels[i].item()],\n",
    "                'predicted_label': fairface_classes[predicted[i].item()],\n",
    "                'confidence': probs[i][predicted[i]].item(),\n",
    "                'top1_pred': fairface_classes[top1_pred[i].item()],\n",
    "                'top1_prob': top1_prob[i].item(),\n",
    "                'top5_preds': [fairface_classes[idx] for idx in top5_preds[i].cpu().numpy()],\n",
    "                'top5_probs': top5_probs[i].cpu().numpy().tolist()\n",
    "            })\n",
    "    # Calculate accuracy\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame(all_predictions)\n",
    "    results_df.to_csv(\"/root/output/quantized_vit_validation_results.csv\", index=False)\n",
    "    \n",
    "    return accuracy, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Local Model Loading and Validation\n",
    "def run_validation():\n",
    "    # Load the quantized ONNX model locally\n",
    "    with open(\"vit_modal-quantized-new.onnx\", \"rb\") as f:  # Update this path to your local model path\n",
    "        model_bytes = f.read()\n",
    "    \n",
    "    # Run validation on Modal\n",
    "    with stub.run():\n",
    "        accuracy, results = validate_quantized_model.remote(\n",
    "            batch_size=32,\n",
    "            model_bytes=model_bytes\n",
    "        )\n",
    "        print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Download results from Modal\n",
    "\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 68.00%\n",
      "Validation Accuracy: 68.00%\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Run Validation\n",
    "accuracy = run_validation()\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation of SWIN quantized INT 8 model on same Validation set from hugging face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Accuracy: 70.88%\n"
     ]
    }
   ],
   "source": [
    "import modal\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import onnxruntime as ort\n",
    "\n",
    "stub = modal.App(\"swin-onnx-validation\")\n",
    "\n",
    "image = modal.Image.debian_slim().pip_install(\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"pandas\",\n",
    "    \"pillow\",\n",
    "    \"pyarrow\",\n",
    "    \"onnxruntime-gpu\"\n",
    ")\n",
    "\n",
    "data_volume = modal.Volume.from_name(\"fairface-data\")\n",
    "model_volume = modal.Volume.from_name(\"vit-quantization-volume\")\n",
    "output_volume = modal.Volume.from_name(\"swin-validation-output\", create_if_missing=True)\n",
    "\n",
    "class FairFaceDataset(Dataset):\n",
    "    def __init__(self, parquet_file, transform=None):\n",
    "        self.data = pd.read_parquet(parquet_file)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image = Image.open(io.BytesIO(row['image']['bytes'])).convert('RGB')\n",
    "        label = row['race']  # Adjust if your label column is different\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "@stub.function(\n",
    "    image=image,\n",
    "    gpu=\"T4\",\n",
    "    timeout=1800,\n",
    "    volumes={\n",
    "        \"/data\": data_volume,\n",
    "        \"/models\": model_volume,\n",
    "        \"/output\": output_volume\n",
    "    }\n",
    ")\n",
    "def validate_swin_onnx(\n",
    "    onnx_model_path=\"/models/swin_fairface_best_int8.onnx\",\n",
    "    parquet_path=\"/data/validation.parquet\",\n",
    "    output_csv=\"/output/swin_val_predictions.csv\",\n",
    "    batch_size=32\n",
    "):\n",
    "    fairface_classes = [\n",
    "        \"White\", \"Black\", \"Latino_Hispanic\", \"East Asian\",\n",
    "        \"Southeast Asian\", \"Indian\", \"Middle Eastern\"\n",
    "    ]\n",
    "    session = ort.InferenceSession(\n",
    "        onnx_model_path,\n",
    "        providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "    )\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "    val_dataset = FairFaceDataset(parquet_path)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    for images, labels in val_loader:\n",
    "        images_np = images.numpy()\n",
    "        outputs = session.run([output_name], {input_name: images_np})[0]\n",
    "        probs = torch.softmax(torch.tensor(outputs), dim=1)\n",
    "        top5_probs, top5_preds = probs.topk(5, dim=1)\n",
    "        top1_pred = top5_preds[:, 0]\n",
    "        top1_prob = top5_probs[:, 0]\n",
    "        _, predicted = probs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        for i in range(len(labels)):\n",
    "            all_predictions.append({\n",
    "                'true_label': fairface_classes[labels[i].item()],\n",
    "                'predicted_label': fairface_classes[predicted[i].item()],\n",
    "                'confidence': probs[i][predicted[i]].item(),\n",
    "                'top1_pred': fairface_classes[top1_pred[i].item()],\n",
    "                'top1_prob': top1_prob[i].item(),\n",
    "                'top5_preds': [fairface_classes[idx] for idx in top5_preds[i].cpu().numpy()],\n",
    "                'top5_probs': top5_probs[i].cpu().numpy().tolist()\n",
    "            })\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "    results_df = pd.DataFrame(all_predictions)\n",
    "    results_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Predictions saved to {output_csv}\")\n",
    "    return accuracy\n",
    "\n",
    "# Run validation\n",
    "with stub.run():\n",
    "    acc = validate_swin_onnx.remote()\n",
    "    print(f\"Final Validation Accuracy: {acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
