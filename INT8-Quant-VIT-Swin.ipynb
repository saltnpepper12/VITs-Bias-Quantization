{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model inputs: ['pixel_values']\n",
      "Model outputs: ['logits']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\py39\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available providers: ['CPUExecutionProvider']\n",
      "Current provider: {'CPUExecutionProvider': {}}\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Check Model Structure\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Load and check the model\n",
    "model = onnx.load(\"vit_fairface_best_quantized_nc.onnx\")\n",
    "print(\"Model inputs:\", [input.name for input in model.graph.input])\n",
    "print(\"Model outputs:\", [output.name for output in model.graph.output])\n",
    "\n",
    "# Try to create a session to verify\n",
    "session = ort.InferenceSession(\n",
    "    \"vit_fairface_best_quantized_nc.onnx\",\n",
    "    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    ")\n",
    "print(\"\\nAvailable providers:\", session.get_providers())\n",
    "print(\"Current provider:\", session.get_provider_options())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIT INT8 SmoothQuant Applied to base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting quantization process...\n",
      "INFO:__main__:Downloading quantized model...\n",
      "ERROR:__main__:Error during process: a bytes-like object is required, not 'generator'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 157\u001b[0m\n\u001b[0;32m    155\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading quantized model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit_fairface_best_quantized_nc.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 157\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_volume\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/output/quantized_model.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuantization successful! Model saved to: vit_fairface_best_quantized_nc.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'generator'"
     ]
    }
   ],
   "source": [
    "import modal\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from neural_compressor import quantization\n",
    "from neural_compressor.config import PostTrainingQuantConfig\n",
    "from neural_compressor.data import DataLoader\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define Modal stub\n",
    "stub = modal.App(\"vit-quantization\")\n",
    "\n",
    "# Define the image for the Modal container\n",
    "image = modal.Image.debian_slim().pip_install(\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"neural-compressor\",\n",
    "    \"onnxruntime-gpu\",\n",
    "    \"pillow\",\n",
    "    \"onnx\"\n",
    ")\n",
    "\n",
    "# Create volumes to store our data\n",
    "fairface_volume = modal.Volume.from_name(\"fairface-data\", create_if_missing=True)\n",
    "output_volume = modal.Volume.from_name(\"vit-quantization-volume\", create_if_missing=True)\n",
    "\n",
    "@stub.function(image=image, gpu=\"T4\", volumes={\"/fairface\": fairface_volume, \"/output\": output_volume})\n",
    "def quantize_model(model_path: str, output_path: str):\n",
    "    logger.info(\"Starting quantization process...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Check if model exists in the volume\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "    else:\n",
    "        logger.info(f\"Found model file at {model_path}, size: {os.path.getsize(model_path)} bytes\")\n",
    "\n",
    "    def create_dataloader(data_dir, batch_size=1):\n",
    "        logger.info(f\"Creating dataloader from directory: {data_dir}\")\n",
    "        if not os.path.exists(data_dir):\n",
    "            raise FileNotFoundError(f\"Calibration directory not found at {data_dir}\")\n",
    "            \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        class Dataset:\n",
    "            def __init__(self, data_dir, transform):\n",
    "                self.data_dir = data_dir\n",
    "                self.transform = transform\n",
    "                self.image_files = [f for f in os.listdir(data_dir) \n",
    "                                  if f.endswith(('.jpg', '.jpeg', '.png', '.webp'))]\n",
    "                logger.info(f\"Found {len(self.image_files)} images for calibration\")\n",
    "                if len(self.image_files) == 0:\n",
    "                    raise ValueError(f\"No calibration images found in {data_dir}\")\n",
    "            \n",
    "            def __len__(self):\n",
    "                return len(self.image_files)\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                image_path = os.path.join(self.data_dir, self.image_files[idx])\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                image = self.transform(image)\n",
    "                return image, 0  # Dummy label\n",
    "        \n",
    "        dataset = Dataset(data_dir, transform)\n",
    "        return DataLoader(framework='pytorch', dataset=dataset, batch_size=batch_size)\n",
    "\n",
    "    # Create dataloader for calibration\n",
    "    calibration_dir = \"/fairface/calibration_images\"\n",
    "    logger.info(\"Creating calibration dataloader...\")\n",
    "    dataloader = create_dataloader(calibration_dir)\n",
    "    \n",
    "    # Configure quantization with simpler settings\n",
    "    logger.info(\"Configuring quantization parameters...\")\n",
    "    conf = PostTrainingQuantConfig(\n",
    "        device='gpu',\n",
    "        backend='onnxrt_cuda_ep',\n",
    "        approach='static',\n",
    "        calibration_sampling_size=[500],\n",
    "        op_type_dict={\n",
    "            'Conv': {\n",
    "                'weight': {'dtype': ['int8']},\n",
    "                'activation': {'dtype': ['uint8']}\n",
    "            },\n",
    "            'MatMul': {\n",
    "                'weight': {'dtype': ['int8']},\n",
    "                'activation': {'dtype': ['uint8']}\n",
    "            },\n",
    "            'Gemm': {\n",
    "                'weight': {'dtype': ['int8']},\n",
    "                'activation': {'dtype': ['uint8']}\n",
    "            }\n",
    "        },\n",
    "        recipes={\n",
    "            'smooth_quant': True,\n",
    "            'smooth_quant_args': {\n",
    "                'alpha': 0.3,\n",
    "                'folding': True\n",
    "            },\n",
    "            'optypes_to_exclude_output_quant': [\n",
    "                'Softmax',\n",
    "                'LayerNormalization',\n",
    "                'Attention',\n",
    "                'Div',\n",
    "                'Mul',\n",
    "                'Add'\n",
    "            ],\n",
    "            'add_qdq_pair_to_weight': True,\n",
    "            'first_conv_or_matmul_quantization': False,\n",
    "            'last_conv_or_matmul_quantization': False,\n",
    "            'pre_post_process_quantization': False\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Perform quantization\n",
    "    logger.info(\"Starting model quantization...\")\n",
    "    q_model = quantization.fit(\n",
    "        model=model_path,\n",
    "        conf=conf,\n",
    "        calib_dataloader=dataloader\n",
    "    )\n",
    "    \n",
    "    if q_model is None:\n",
    "        raise RuntimeError(\"Quantization failed - no model was returned\")\n",
    "    \n",
    "    # Save the quantized model\n",
    "    logger.info(f\"Saving quantized model to: {output_path}\")\n",
    "    q_model.save(output_path)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Quantization completed in {end_time - start_time:.2f} seconds\")\n",
    "    return output_path\n",
    "\n",
    "# Run the quantization\n",
    "with stub.run():\n",
    "    logger.info(\"Starting quantization process...\")\n",
    "    try:\n",
    "        # Run quantization using the model from the volume\n",
    "        remote_output_path = quantize_model.remote(\n",
    "            \"/output/model.onnx\",  # Path to the model in the volume\n",
    "            \"/output/quantized_model.onnx\"\n",
    "        )\n",
    "        \n",
    "        # Download the result\n",
    "        logger.info(\"Downloading quantized model...\")\n",
    "        with open(\"vit_fairface_best_quantized_nc.onnx\", 'wb') as f:\n",
    "            f.write(output_volume.read_file(\"/output/quantized_model.onnx\"))\n",
    "        \n",
    "        logger.info(\"Quantization successful! Model saved to: vit_fairface_best_quantized_nc.onnx\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during process: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore above error , The code still works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic INT8 Quantization of Swin Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType\n",
    "\n",
    "stub = modal.App(\"swin-onnx-int8-quant\")\n",
    "\n",
    "image = modal.Image.debian_slim().pip_install(\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"onnxruntime-gpu\",\n",
    "    \"onnx\",\n",
    "    \"pillow\"\n",
    ")\n",
    "\n",
    "fairface_volume = modal.Volume.from_name(\"fairface-data\", create_if_missing=True)\n",
    "output_volume = modal.Volume.from_name(\"vit-quantization-volume\", create_if_missing=True)\n",
    "swinv2_model_volume = modal.Volume.from_name(\"swinv2-models\", create_if_missing=True)\n",
    "\n",
    "class ImageFolderDataReader(CalibrationDataReader):\n",
    "    def __init__(self, image_dir, input_name=\"input\"):\n",
    "        self.image_paths = [\n",
    "            os.path.join(image_dir, f)\n",
    "            for f in os.listdir(image_dir)\n",
    "            if f.lower().endswith(('.jpg', '.jpeg', '.png', '.webp'))\n",
    "        ]\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.idx = 0\n",
    "        self.input_name = input_name\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.idx >= len(self.image_paths):\n",
    "            return None\n",
    "        image = Image.open(self.image_paths[self.idx]).convert('RGB')\n",
    "        input_tensor = self.transform(image).unsqueeze(0).numpy()\n",
    "        self.idx += 1\n",
    "        return {self.input_name: input_tensor}\n",
    "\n",
    "    def rewind(self):\n",
    "        self.idx = 0\n",
    "\n",
    "@stub.function(\n",
    "    image=image,\n",
    "    gpu=\"T4\",\n",
    "    timeout=1800,\n",
    "    volumes={\n",
    "        \"/fairface\": fairface_volume,\n",
    "        \"/output\": output_volume,\n",
    "        \"/models\": swinv2_model_volume,\n",
    "    }\n",
    ")\n",
    "def quantize_swin_onnx(\n",
    "    onnx_model_path=\"/models/swin_fairface_best.onnx\",\n",
    "    quantized_model_path=\"/output/swin_fairface_best_int8.onnx\",\n",
    "    calibration_dir=\"/fairface/calibration_images\"\n",
    "):\n",
    "    import onnx\n",
    "    # Get the correct input name for your model\n",
    "    m = onnx.load(onnx_model_path)\n",
    "    input_name = m.graph.input[0].name\n",
    "\n",
    "    data_reader = ImageFolderDataReader(calibration_dir, input_name=input_name)\n",
    "    print(f\"Quantizing {onnx_model_path} to {quantized_model_path} using {len(data_reader.image_paths)} calibration images...\")\n",
    "\n",
    "    quantize_static(\n",
    "        model_input=onnx_model_path,\n",
    "        model_output=quantized_model_path,\n",
    "        calibration_data_reader=data_reader,\n",
    "        weight_type=QuantType.QInt8,\n",
    "        activation_type=QuantType.QUInt8\n",
    "     \n",
    "    )\n",
    "    print(f\"Quantized model saved to {quantized_model_path}\")\n",
    "\n",
    "# Run the quantization\n",
    "with stub.run():\n",
    "    quantize_swin_onnx.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
